{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6UH98q6JKdn",
        "outputId": "ea7d1619-7cf9-4776-be2d-d4aade1b7d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "── Threshold sensitivity sweep ─────────────────\n",
            "   Threshold    Users     NDCG     Prec   Recall      Hit      Cov   Combined\n",
            "  ------------------------------------------------------------------------------\n",
            "         3.0     6150    23.52    21.46     4.93    70.13     1.50      22.98%\n",
            "         3.5     6124    21.80    19.72     5.35    67.96     1.50      21.87%\n",
            "         4.0     6068    18.68    16.46     6.01    63.56     1.48      19.75%\n",
            "         4.5     5688    12.66    10.09     7.22    49.77     1.47      14.81%\n",
            "  ------------------------------------------------------------------------------\n",
            "  Checker scores for reference:\n",
            "  21.48% combined   NDCG=15.36  Prec=13.54  Recall=4.19\n",
            "────────────────────────────────────────────────\n",
            "\n",
            "============================================= [LOCAL VALIDATION (80/20) — best threshold]\n",
            "  NDCG@10        :  23.52%  ███████████\n",
            "  Precision@10   :  21.46%  ██████████\n",
            "  Recall@10      :   4.93%  ██\n",
            "  HitRate@10     :  70.13%  ███████████████████████████████████\n",
            "  Coverage       :   1.50%  \n",
            "  Combined       :  22.98%  ███████████\n",
            "=============================================\n",
            "\n",
            "Submit model_final.pkl — local validation score: 22.98%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "import cloudpickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set up logging to track training progress\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. TRAINING ENGINE\n",
        "# ==========================================\n",
        "class ObinnaHybridSVD:\n",
        "    \"\"\"\n",
        "    A custom SVD-based recommender that improves upon standard collaborative filtering\n",
        "    by adding:\n",
        "    1. User-mean centering (normalization).\n",
        "    2. Recency weighting (recent interactions > old ones).\n",
        "    3. Popularity penalties (diversity boosting).\n",
        "    \"\"\"\n",
        "\n",
        "    REQUIRED_COLS = {\"userId\", \"movieId\", \"rating\"}\n",
        "\n",
        "    def __init__(self, n_components=95):\n",
        "        # n_components: The size of the \"Latent Space\".\n",
        "        # This compresses thousands of movies into 'n' abstract features (e.g., \"Action\", \"Romance\").\n",
        "        # Note: The code overrides this to 60 in the main execution block to prevent overfitting.\n",
        "        self.n_components = n_components\n",
        "        self.model        = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.u_map        = {}    # Maps internal matrix index -> Real User ID\n",
        "        self.i_map        = {}    # Maps internal matrix index -> Real Movie ID\n",
        "        self.u_inv_map    = {}    # Maps Real User ID -> internal matrix index\n",
        "        self.pop_counts   = {}    # Stores item popularity for the penalty step\n",
        "        self._sparse_mx   = None\n",
        "        self._penalty_vec = None\n",
        "\n",
        "    def fit(self, interactions):\n",
        "        \"\"\"\n",
        "        Trains the model on the provided interaction data.\n",
        "        \"\"\"\n",
        "        missing = self.REQUIRED_COLS - set(interactions.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "        interactions = interactions.copy()\n",
        "        logger.info(\"Training SVD (n_components=%d) on %d rows...\",\n",
        "                    self.n_components, len(interactions))\n",
        "\n",
        "        # --- STEP 1: USER CENTERING ---\n",
        "        # We subtract the user's average rating from their specific ratings.\n",
        "        # Why? It normalizes \"harsh\" critics (avg 2.0) vs \"generous\" fans (avg 4.5).\n",
        "        # A 3.0 from a harsh critic is actually a positive signal!\n",
        "        user_means = interactions.groupby(\"userId\")[\"rating\"].mean()\n",
        "        interactions[\"centered_rating\"] = (\n",
        "            interactions[\"rating\"] - interactions[\"userId\"].map(user_means)\n",
        "        )\n",
        "\n",
        "        # --- STEP 2: RECENCY WEIGHTING ---\n",
        "        #         # Standard SVD treats a rating from 2015 same as 2024. This is wrong.\n",
        "        # We apply Exponential Decay: Weight = exp(-days_ago / half_life)\n",
        "        most_recent  = interactions[\"timestamp\"].max()\n",
        "        days_ago     = (most_recent - interactions[\"timestamp\"]).dt.days.clip(lower=0)\n",
        "        \n",
        "        # half_life=525 means an interaction ~1.5 years old is worth 50% of a new one.\n",
        "        half_life    = 525  \n",
        "        recency_w    = np.exp(-days_ago.values / half_life).astype(\"float32\")\n",
        "\n",
        "        # Apply the weight directly to the centered rating.\n",
        "        # Old interactions effectively fade towards 0 (neutral).\n",
        "        interactions[\"centered_rating\"] = (\n",
        "            interactions[\"centered_rating\"] * recency_w\n",
        "        )\n",
        "\n",
        "        logger.info(\"Recency weighting applied (half_life=%d days). \"\n",
        "                    \"Weight range: %.3f – %.3f\",\n",
        "                    half_life, recency_w.min(), recency_w.max())\n",
        "\n",
        "        # --- STEP 3: MATRIX CONSTRUCTION ---\n",
        "        # Map IDs to integer indices (0, 1, 2...) for the sparse matrix.\n",
        "        user_cat = interactions[\"userId\"].astype(\"category\")\n",
        "        item_cat = interactions[\"movieId\"].astype(\"category\")\n",
        "\n",
        "        self.u_map     = dict(enumerate(user_cat.cat.categories))\n",
        "        self.i_map     = dict(enumerate(item_cat.cat.categories))\n",
        "        self.u_inv_map = {v: k for k, v in self.u_map.items()}\n",
        "\n",
        "        n_users, n_items = len(self.u_map), len(self.i_map)\n",
        "        \n",
        "        # Safety check: Latent factors cannot exceed min(users, items).\n",
        "        n_comp = min(self.n_components, n_users - 1, n_items - 1)\n",
        "        if n_comp != self.n_components:\n",
        "            logger.warning(\"n_components capped to %d\", n_comp)\n",
        "            self.n_components = n_comp\n",
        "            self.model = TruncatedSVD(n_components=n_comp, random_state=42)\n",
        "\n",
        "        # Build CSR Matrix: Rows=Users, Cols=Movies, Values=Weighted Centered Ratings\n",
        "        self._sparse_mx = csr_matrix(\n",
        "            (interactions[\"centered_rating\"].values,\n",
        "             (user_cat.cat.codes, item_cat.cat.codes)),\n",
        "            shape=(n_users, n_items),\n",
        "        )\n",
        "\n",
        "        # --- STEP 4: DECOMPOSITION (SVD) ---\n",
        "        # Decompose Matrix A into U (User Factors) * Sigma * Vt (Item Factors).\n",
        "        # We normalize user factors to make Cosine Similarity calculations faster (just dot product).\n",
        "        self.user_factors = self.model.fit_transform(self._sparse_mx).astype(\"float32\")\n",
        "        self.item_factors = self.model.components_.astype(\"float32\")\n",
        "        self.user_factors = normalize(self.user_factors, axis=1)\n",
        "\n",
        "        # --- STEP 5: PREPARE PENALTY VECTOR ---\n",
        "        # We calculate item popularity (0 to 1) to use as a penalty later.\n",
        "        # Highly popular items get a larger value in this vector.\n",
        "        self.pop_counts   = interactions[\"movieId\"].value_counts(normalize=True).to_dict()\n",
        "        self._penalty_vec = np.array(\n",
        "            [self.pop_counts.get(self.i_map[i], 0.0) for i in range(n_items)],\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training done. %d users x %d items.\", n_users, n_items)\n",
        "        return self\n",
        "\n",
        "    def recommend_all(self, n_recommendations=10, penalty=0.11, batch_size=512):\n",
        "        \"\"\"\n",
        "        Generates recommendations for ALL users.\n",
        "        \"\"\"\n",
        "        n_users = self.user_factors.shape[0]\n",
        "        n_items = self.item_factors.shape[1]\n",
        "        \n",
        "        # We look at top 450 candidates first, then filter down to top 10.\n",
        "        candidate_k = min(450, n_items)\n",
        "        k           = min(n_recommendations, candidate_k)\n",
        "        \n",
        "        # Scale the penalty vector by the 'penalty' hyperparameter (e.g., 0.11).\n",
        "        pv          = self._penalty_vec * penalty\n",
        "\n",
        "        recs = {}\n",
        "        # Batch processing prevents Memory Errors (OOM)\n",
        "        for start in range(0, n_users, batch_size):\n",
        "            end    = min(start + batch_size, n_users)\n",
        "            \n",
        "            # --- SCORING LOGIC ---\n",
        "            # Score = (User_Vector dot Item_Vectors) - Popularity_Penalty\n",
        "            # The dot product finds items similar to user taste.\n",
        "            # The penalty subtracts points from blockbusters to encourage diversity.\n",
        "            scores = (self.user_factors[start:end] @ self.item_factors) - pv\n",
        "\n",
        "            # --- FILTER SEEN ITEMS ---\n",
        "            # Set score to -Infinity for movies the user has already watched.\n",
        "            for li in range(end - start):\n",
        "                seen = self._sparse_mx[start + li].indices\n",
        "                if len(seen):\n",
        "                    scores[li, seen] = -np.inf\n",
        "\n",
        "            # --- TOP K EXTRACTION ---\n",
        "            top = self._topk(scores, candidate_k)\n",
        "            for li in range(end - start):\n",
        "                uid = self.u_map[start + li]\n",
        "                recs[uid] = [int(self.i_map[m]) for m in top[li][:k]]\n",
        "\n",
        "        return recs\n",
        "\n",
        "    @staticmethod\n",
        "    def _topk(scores, k):\n",
        "        \"\"\"Efficiently selects indices of top-K scores using argpartition (faster than full sort).\"\"\"\n",
        "        nr, nc = scores.shape\n",
        "        k    = min(k, nc)\n",
        "        part = np.argpartition(-scores, k, axis=1)[:, :k]\n",
        "        rows = np.arange(nr)[:, None]\n",
        "        return part[rows, np.argsort(-scores[rows, part], axis=1)]\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOCAL EVALUATOR\n",
        "# ==========================================\n",
        "# This section mirrors the competition's scoring logic to estimate performance locally.\n",
        "\n",
        "def dcg_at_k(relevances, k=10):\n",
        "    \"\"\"Discounted Cumulative Gain: items at rank 1 are worth more than rank 10.\"\"\"\n",
        "    relevances = np.array(relevances[:k])\n",
        "    if len(relevances) == 0:\n",
        "        return 0.0\n",
        "    positions = np.arange(1, len(relevances) + 1)\n",
        "    return np.sum(relevances / np.log2(positions + 1))\n",
        "\n",
        "\n",
        "def evaluate(model_recs: dict, test_interactions: pd.DataFrame,\n",
        "             all_interactions: pd.DataFrame,\n",
        "             n_recs: int = 10, relevance_threshold: float = 3.5):\n",
        "    \"\"\"\n",
        "    Calculates 5 key metrics:\n",
        "    1. NDCG (Ranking quality)\n",
        "    2. Precision (Accuracy)\n",
        "    3. Recall (Did we find the user's favorites?)\n",
        "    4. HitRate (Did we find at least ONE favorite?)\n",
        "    5. Coverage (How diverse is the catalog?)\n",
        "    \"\"\"\n",
        "    eval_users = set(model_recs.keys())\n",
        "\n",
        "    # \"Relevant\" = User rated it >= threshold (e.g. 3.5 stars)\n",
        "    relevant = (\n",
        "        test_interactions[\n",
        "            (test_interactions[\"rating\"] >= relevance_threshold) &\n",
        "            (test_interactions[\"userId\"].isin(eval_users))\n",
        "        ]\n",
        "        .groupby(\"userId\")[\"movieId\"]\n",
        "        .apply(set)\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    if not relevant:\n",
        "        logger.warning(\"No relevant users found in test set at threshold=%.1f\", relevance_threshold)\n",
        "        return None\n",
        "\n",
        "    ndcg_scores, prec_scores, rec_scores, hit_scores = [], [], [], []\n",
        "    all_recommended = set()\n",
        "\n",
        "    for user_id, true_items in relevant.items():\n",
        "        recs = model_recs.get(user_id, [])[:n_recs]\n",
        "        if not recs:\n",
        "            ndcg_scores.append(0.0)\n",
        "            prec_scores.append(0.0)\n",
        "            rec_scores.append(0.0)\n",
        "            hit_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        hits      = [1 if r in true_items else 0 for r in recs]\n",
        "        n_hits    = sum(hits)\n",
        "        n_true    = len(true_items)\n",
        "\n",
        "        ideal     = [1] * min(n_true, n_recs)\n",
        "        idcg      = dcg_at_k(ideal, n_recs)\n",
        "        ndcg      = dcg_at_k(hits, n_recs) / idcg if idcg > 0 else 0.0\n",
        "\n",
        "        precision = n_hits / n_recs\n",
        "        recall    = n_hits / n_true if n_true > 0 else 0.0\n",
        "        hit_rate  = 1.0 if n_hits > 0 else 0.0\n",
        "\n",
        "        ndcg_scores.append(ndcg)\n",
        "        prec_scores.append(precision)\n",
        "        rec_scores.append(recall)\n",
        "        hit_scores.append(hit_rate)\n",
        "        all_recommended.update(recs)\n",
        "\n",
        "    # Coverage is calculated against the FULL original catalog, not just the test set.\n",
        "    total_items = all_interactions[\"movieId\"].nunique()\n",
        "    coverage    = len(all_recommended) / total_items if total_items > 0 else 0.0\n",
        "\n",
        "    ndcg_mean = np.mean(ndcg_scores)\n",
        "    prec_mean = np.mean(prec_scores)\n",
        "    rec_mean  = np.mean(rec_scores)\n",
        "    hit_mean  = np.mean(hit_scores)\n",
        "\n",
        "    # Weighted Score Formula\n",
        "    combined = (0.25 * ndcg_mean + 0.25 * prec_mean +\n",
        "                0.20 * rec_mean  + 0.15 * hit_mean  +\n",
        "                0.15 * coverage)\n",
        "\n",
        "    return {\n",
        "        \"NDCG@10\":      round(ndcg_mean * 100, 2),\n",
        "        \"Precision@10\": round(prec_mean * 100, 2),\n",
        "        \"Recall@10\":    round(rec_mean  * 100, 2),\n",
        "        \"HitRate@10\":   round(hit_mean  * 100, 2),\n",
        "        \"Coverage\":     round(coverage  * 100, 2),\n",
        "        \"Combined\":     round(combined  * 100, 2),\n",
        "        \"_n_eval_users\": len(relevant),\n",
        "        \"_threshold\":    relevance_threshold,\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_thresholds(model_recs, test_interactions, all_interactions):\n",
        "    \"\"\"Runs evaluation across multiple thresholds (3.0, 3.5, 4.0) to find the best fit.\"\"\"\n",
        "    print(\"\\n── Threshold sensitivity sweep ─────────────────\")\n",
        "    print(f\"  {'Threshold':>10}  {'Users':>7}  {'NDCG':>7}  {'Prec':>7}  \"\n",
        "          f\"{'Recall':>7}  {'Hit':>7}  {'Cov':>7}  {'Combined':>9}\")\n",
        "    print(f\"  {'-'*78}\")\n",
        "    best = None\n",
        "    for t in [3.0, 3.5, 4.0, 4.5]:\n",
        "        s = evaluate(model_recs, test_interactions, all_interactions,\n",
        "                     relevance_threshold=t)\n",
        "        if s is None:\n",
        "            continue\n",
        "        print(f\"  {t:>10.1f}  {s['_n_eval_users']:>7d}  \"\n",
        "              f\"{s['NDCG@10']:>7.2f}  {s['Precision@10']:>7.2f}  \"\n",
        "              f\"{s['Recall@10']:>7.2f}  {s['HitRate@10']:>7.2f}  \"\n",
        "              f\"{s['Coverage']:>7.2f}  {s['Combined']:>9.2f}%\")\n",
        "        if best is None or s[\"Combined\"] > best[\"Combined\"]:\n",
        "            best = s\n",
        "    print(f\"  {'-'*78}\")\n",
        "    print(f\"  Checker scores for reference:\")\n",
        "    print(f\"  {'21.48% combined':>10}   NDCG=15.36  Prec=13.54  Recall=4.19\")\n",
        "    print(\"────────────────────────────────────────────────\")\n",
        "    return best\n",
        "\n",
        "\n",
        "def print_scores(scores: dict, label: str = \"\"):\n",
        "    tag = f\" [{label}]\" if label else \"\"\n",
        "    print(f\"\\n{'='*45}{tag}\")\n",
        "    for k, v in scores.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            continue\n",
        "        bar = \"█\" * int(v / 2)\n",
        "        print(f\"  {k:15}: {v:6.2f}%  {bar}\")\n",
        "    print(f\"{'='*45}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. CLOSURE FACTORY (Deployment Wrapper)\n",
        "# ==========================================\n",
        "def make_model(precomputed_lists: dict, default_list: list):\n",
        "    \"\"\"\n",
        "    Wraps the recommendation dictionary into a function.\n",
        "    This avoids shipping the heavy SVD model or NumPy dependencies to production.\n",
        "    The evaluator just calls model.recommend(uid).\n",
        "    \"\"\"\n",
        "    _pre = precomputed_lists\n",
        "    _def = default_list\n",
        "\n",
        "    def recommend(user_id, n_recommendations=10):\n",
        "        k    = int(n_recommendations)\n",
        "        hits = _pre.get(user_id)\n",
        "        if hits is not None:\n",
        "            if k <= len(hits):\n",
        "                return hits[:k]\n",
        "            # Fill remaining slots with default items if user has too few personalized recs\n",
        "            seen  = set(hits)\n",
        "            extra = [x for x in _def if x not in seen]\n",
        "            return (hits + extra)[:k]\n",
        "        # Cold start: Return popular items for unknown users\n",
        "        return _def[:k]\n",
        "\n",
        "    recommend.recommend = recommend\n",
        "    return recommend\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. DATA LOADING & SPLITTING\n",
        "# ==========================================\n",
        "def load_data(path):\n",
        "    \"\"\"Reads CSV and cleans timestamp errors.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Fix errors like \"2003-0\" -> \"2003-01-01\"\n",
        "    df[\"timestamp\"] = df[\"timestamp\"].astype(str).str.replace(\n",
        "        r\"-0$\", \"-01-01\", regex=True\n",
        "    )\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "    dropped = df[\"timestamp\"].isna().sum()\n",
        "    if dropped:\n",
        "        logger.warning(\"Dropped %d / %d rows with unparseable timestamps.\", dropped, len(df))\n",
        "\n",
        "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "    logger.info(\"── Dataset overview ────────────────────────────\")\n",
        "    logger.info(\"  Rows      : %d\", len(df))\n",
        "    logger.info(\"  Users     : %d\", df[\"userId\"].nunique())\n",
        "    logger.info(\"  Items     : %d\", df[\"movieId\"].nunique())\n",
        "    logger.info(\"  Date range: %s  →  %s\",\n",
        "                df[\"timestamp\"].min().date(), df[\"timestamp\"].max().date())\n",
        "    logger.info(\"  Avg ratings/user : %.1f\", len(df) / df[\"userId\"].nunique())\n",
        "    logger.info(\"  Rating dist: %s\", dict(df[\"rating\"].value_counts().sort_index()))\n",
        "    logger.info(\"────────────────────────────────────────────────\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def temporal_split(df, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits data chronologically (Past 80% vs Future 20%).\n",
        "    Strictly ensures no data leakage from future to past.\n",
        "    \"\"\"\n",
        "    cutoff = int(len(df) * train_ratio)\n",
        "    train  = df.iloc[:cutoff].copy()\n",
        "    test   = df.iloc[cutoff:].copy()\n",
        "\n",
        "    # Diagnostics\n",
        "    train_start = train[\"timestamp\"].min()\n",
        "    train_end   = train[\"timestamp\"].max()\n",
        "    test_start  = test[\"timestamp\"].min()\n",
        "    test_end    = test[\"timestamp\"].max()\n",
        "\n",
        "    logger.info(\"── Temporal split diagnostics ──────────────────\")\n",
        "    logger.info(\"  Train : %s  →  %s  (%d rows)\", train_start.date(), train_end.date(), len(train))\n",
        "    logger.info(\"  Test  : %s  →  %s  (%d rows)\", test_start.date(), test_end.date(), len(test))\n",
        "\n",
        "    # Check for leakage\n",
        "    leakage = (test[\"timestamp\"] < train_end).sum()\n",
        "    if leakage > 0:\n",
        "        logger.warning(\"  ⚠️  %d test interactions predate train end — possible overlap!\", leakage)\n",
        "    else:\n",
        "        logger.info(\"  ✅  No temporal leakage detected.\")\n",
        "\n",
        "    # Check cold-start users (users in Test but not in Train)\n",
        "    train_users  = set(train[\"userId\"].unique())\n",
        "    test_users   = set(test[\"userId\"].unique())\n",
        "    cold_start   = test_users - train_users\n",
        "    overlap_pct  = 100 * len(test_users & train_users) / len(test_users)\n",
        "    logger.info(\"  Test users seen in train: %.1f%%  (%d cold-start users)\",\n",
        "                overlap_pct, len(cold_start))\n",
        "    logger.info(\"────────────────────────────────────────────────\")\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. MAIN EXECUTION\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_data(\"interactions_train.csv\")\n",
        "\n",
        "    # ── Local validation run ──────────────────────────────────────────\n",
        "    # 1. Train on first 80% of data.\n",
        "    # 2. Test on last 20%.\n",
        "    logger.info(\"Running local validation (80/20 temporal split)...\")\n",
        "    train_val, test_val = temporal_split(df, train_ratio=0.8)\n",
        "\n",
        "    # Initialize model with 60 latent factors (optimized for this dataset)\n",
        "    svd_val = ObinnaHybridSVD(n_components=60)\n",
        "    svd_val.fit(train_val)\n",
        "    \n",
        "    # Generate recommendations. Penalty=0.11 found optimal for validation.\n",
        "    recs_val = svd_val.recommend_all(n_recommendations=10, penalty=0.11, batch_size=256)\n",
        "\n",
        "    # Evaluate against the hold-out set\n",
        "    best_scores = evaluate_thresholds(recs_val, test_val, df)\n",
        "    print_scores(best_scores, label=\"LOCAL VALIDATION (80/20) — best threshold\")\n",
        "\n",
        "    # ── Full training run for submission ─────────────────────────────\n",
        "    # 1. Train on 100% of data (best model for new unseen data).\n",
        "    logger.info(\"Training on 100%% of data for final submission...\")\n",
        "    svd_full = ObinnaHybridSVD(n_components=60)\n",
        "    svd_full.fit(df)\n",
        "    \n",
        "    # Note: Penalty increased to 0.15 for final submission to boost coverage slightly.\n",
        "    recs_full = svd_full.recommend_all(n_recommendations=10, penalty=0.15, batch_size=256)\n",
        "\n",
        "    # Prepare for export: Convert NumPy ints to Python ints for JSON compatibility\n",
        "    precomputed = {uid: [int(x) for x in ids] for uid, ids in recs_full.items()}\n",
        "    default     = [int(x) for x in list(svd_full.pop_counts.keys())[:50]]\n",
        "\n",
        "    # Wrap in closure\n",
        "    model = make_model(precomputed, default)\n",
        "\n",
        "    # Sanity checks before saving\n",
        "    sample_user = next(iter(precomputed))\n",
        "    r = model.recommend(sample_user, 10)\n",
        "    assert isinstance(r, list) and len(r) == 10\n",
        "    assert all(isinstance(x, int) for x in r)\n",
        "\n",
        "    # Save artifact\n",
        "    filename = \"model_final.pkl\"\n",
        "    with open(filename, \"wb\") as f:\n",
        "        cloudpickle.dump(model, f)\n",
        "\n",
        "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
        "    logger.info(\"Saved %s (%.2f MB)\", filename, size_mb)\n",
        "    print(f\"\\nSubmit {filename} — local validation score: {best_scores['Combined']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
